# ğŸ“º YouTube Data Engineering Pipeline â€” MrBeast Channel

[![Python](https://img.shields.io/badge/Python-3.10+-blue)](https://www.python.org/)
[![Airflow](https://img.shields.io/badge/Airflow-2.x-brightgreen)](https://airflow.apache.org/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15-blue)](https://www.postgresql.org/)
[![Docker](https://img.shields.io/badge/Docker-Compose-blue)](https://www.docker.com/)
[![CI/CD](https://img.shields.io/badge/CI%2FCD-GitHub%20Actions-black)](https://github.com/features/actions)

A **production-style data engineering pipeline** that extracts YouTube data from the **MrBeast channel**, transforms it with business logic and AI-powered sentiment analysis, validates data quality, and deploys everything with **Airflow, Docker, Soda, and GitHub Actions CI/CD**.

---

## ğŸ“Œ Table of Contents
- [Project Overview](#project-overview)
- [Architecture Overview](#architecture-overview)
- [Tech Stack](#tech-stack)
- [Pipeline Workflow](#pipeline-workflow)
- [Airflow DAG Flow](#airflow-dag-flow)
- [Project Structure](#project-structure)
- [Data Quality & Testing](#data-quality--testing)
- [CI/CD Workflow](#cicd-workflow)
- [Environment & Secrets](#environment--secrets)
- [How to Run](#how-to-run)

---

## ğŸ§  Project Overview

This project builds an **end-to-end ELT pipeline** that:

### âœ… Extracts
- Data from the **YouTube Data API**
- Targets the **MrBeast channel**
- Collects:
  - Video ID
  - Title
  - Duration
  - View count
  - Like count
  - Comment count
  - Publish date
- Stores raw data as **JSON (Bronze layer)**

### âœ… Loads
- Uses **PostgreSQL** as the data warehouse
- Creates **staging and core tables**

### âœ… Transforms
- Classifies videos into:
  - `short`
  - `normal`
- Adds **AI-powered sentiment analysis** on video titles using **HuggingFace**
- Converts YouTube `PT` duration format into proper timestamps

### âœ… Orchestrates
- Uses **Apache Airflow** with:
  - Scheduler
  - Webserver
  - Workers
  - Redis
  - PostgreSQL metadata DB

### âœ… Validates
- Uses **Soda SQL** for data quality checks

### âœ… Tests & Deploys
- Unit, integration, and end-to-end tests with **Pytest**
- CI/CD automation using **GitHub Actions**

---

## ğŸ— Architecture Overview

```
YouTube API
   â†“
Raw JSON (Bronze)
   â†“
PostgreSQL Staging
   â†“
Transformations + AI Sentiment
   â†“
PostgreSQL Core Tables
   â†“
Soda Data Quality Checks
   â†“
Airflow DAGs
   â†“
CI/CD (GitHub Actions)

```

## ğŸ›  Tech Stack
- Language: Python

- API: YouTube Data API v3

- Orchestration: Apache Airflow

- Warehouse: PostgreSQL

- Containerization: Docker & Docker Compose

- AI / NLP: HuggingFace Transformers

- Data Quality: Soda SQL

- Testing: Pytest

- CI/CD: GitHub Actions


## âš™ï¸ Pipeline Workflow
```
Extract YouTube Data
       â†“
Fetch all video IDs from MrBeast channel
       â†“
Pull video metadata
       â†“
Save raw JSON
       â†“
Database Setup
      â†“
PostgreSQL initialized via Docker
       â†“
Staging and core tables created via hooks and cursors
       â†“
Transformation Layer
       â†“
Video duration â†’ short / normal
       â†“
Title sentiment + sentiment score
       â†“
Timestamp normalization
       â†“
Modification Layer
       â†“
Insert, update, delete logic
       â†“
Uses row dictionaries returned from transformations
       â†“
Orchestration
      â†“
Tasks defined and grouped in Airflow DAGs
       â†“
Monitored via Airflow UI
       â†“
Data Quality Checks
       â†“
Soda scans executed post-load

```
## ğŸ—‚ Airflow DAG Flow

### flowchart TD
    A[Trigger DAG] --> B[Extract YouTube API Data]
    B --> C[Save Raw JSON]
    C --> D[Load to PostgreSQL Staging]
    D --> E[Transform Data]
    E --> F[Video Type Logic]
    E --> G[Sentiment Analysis]
    F --> H[Core Table Insert / Update]
    G --> H
    H --> I[Soda Data Quality Scan]
    I --> J[Unit / Integration / E2E Tests]
    J --> K[DAG Success]

## ğŸ“‚ Project Structure
```
.
â”œâ”€â”€ dags/
    â”œâ”€â”€ api
    â”‚   â”œâ”€â”€ video_api.py
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ datawarehousing/
â”‚   â”œâ”€â”€ data_utils.py          # DB hooks, connections, AI sentiment
â”‚   â”œâ”€â”€ data_loading.py        # API extraction & raw loading
â”‚   â”œâ”€â”€ transformation.py     # Business + AI transformations
â”‚   â”œâ”€â”€ modification.py       # Insert / update / delete logic
â”œâ”€â”€ data_quality/
     â”œâ”€â”€ soda_testing.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â”œâ”€â”€ e2e/
â”‚   â””â”€â”€ conftest.py
â”œâ”€â”€ soda/
â”‚   â”œâ”€â”€ staging_checks.yml
    â”œâ”€â”€ corechecks.yml
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ§ª Data Quality & Testing
**ğŸ” Soda Checks**
```
No duplicate records

No missing critical columns

â‰¥ 90% sentiment scores not zero

â‰¥ 90% titles not neutral
```

**ğŸ§ª Testing Levels**
- Unit Tests: DAG imports, mocks, DB connections

- Integration Tests: Real API & PostgreSQL

- End-to-End Tests: Full pipeline execution via pytest
- 

## ğŸš€ CI/CD Workflow
```
- Implemented using GitHub Actions

- Conditional workflows based on file changes:

- requirements.txt

- DAGs

- Soda configs

- Docker files

- Supports manual workflow dispatch
```

## ğŸ” Environment & Secrets
- .env files are not committed

- Secrets stored in GitHub Secrets & Variables

**Referenced as:**
```
yaml
Copy code
${{ secrets.SECRET_NAME }}
Docker Compose refactored to read from GitHub secrets
```
## âš¡ How to Run
- Clone the repository

- Run docker-compose up -d

- Open Airflow UI at http://localhost:8080

- Trigger DAG

- Run pytest for validation

## âœ… Final Outcome
- âœ” End-to-end YouTube ELT pipeline
- âœ” AI-enhanced analytics
- âœ” Production-grade Airflow orchestration
- âœ” Automated data quality & CI/CD

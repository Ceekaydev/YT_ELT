# ğŸ“º YouTube Data Engineering Pipeline â€” MrBeast Channel

[![Python](https://img.shields.io/badge/Python-3.10+-blue)](https://www.python.org/)
[![Airflow](https://img.shields.io/badge/Airflow-2.x-brightgreen)](https://airflow.apache.org/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15-blue)](https://www.postgresql.org/)
[![Docker](https://img.shields.io/badge/Docker-Compose-blue)](https://www.docker.com/)
[![CI/CD](https://img.shields.io/badge/CI%2FCD-GitHub%20Actions-black)](https://github.com/features/actions)

A **production-style data engineering pipeline** that extracts YouTube data from the **MrBeast channel**, transforms it with business logic and AI-powered sentiment analysis, validates data quality, and deploys everything with **Airflow, Docker, Soda, and GitHub Actions CI/CD**.

---

## ğŸ“Œ Table of Contents
- [Project Overview](#project-overview)
- [Architecture Overview](#architecture-overview)
- [Tech Stack](#tech-stack)
- [Pipeline Workflow](#pipeline-workflow)
- [Airflow DAG Flow](#airflow-dag-flow)
- [Project Structure](#project-structure)
- [Data Quality & Testing](#data-quality--testing)
- [CI/CD Workflow](#cicd-workflow)
- [Environment & Secrets](#environment--secrets)
- [How to Run](#how-to-run)

---

## ğŸ§  Project Overview

This project builds an **end-to-end ELT pipeline** that:

### âœ… Extracts
- Data from the **YouTube Data API**
- Targets the **MrBeast channel**
- Collects:
  - Video ID
  - Title
  - Duration
  - View count
  - Like count
  - Comment count
  - Publish date
- Stores raw data as **JSON (Bronze layer)**

### âœ… Loads
- Uses **PostgreSQL** as the data warehouse
- Creates **staging and core tables**

### âœ… Transforms
- Classifies videos into:
  - `short`
  - `normal`
- Adds **AI-powered sentiment analysis** on video titles using **HuggingFace**
- Converts YouTube `PT` duration format into proper timestamps

### âœ… Orchestrates
- Uses **Apache Airflow** with:
  - Scheduler
  - Webserver
  - Workers
  - Redis
  - PostgreSQL metadata DB

### âœ… Validates
- Uses **Soda SQL** for data quality checks

### âœ… Tests & Deploys
- Unit, integration, and end-to-end tests with **Pytest**
- CI/CD automation using **GitHub Actions**

---

## ğŸ— Architecture Overview

```text
YouTube API
   â†“
Raw JSON (Bronze)
   â†“
PostgreSQL Staging
   â†“
Transformations + AI Sentiment
   â†“
PostgreSQL Core Tables
   â†“
Soda Data Quality Checks
   â†“
Airflow DAGs
   â†“
CI/CD (GitHub Actions)
ğŸ›  Tech Stack
Language: Python

API: YouTube Data API v3

Orchestration: Apache Airflow

Warehouse: PostgreSQL

Containerization: Docker & Docker Compose

AI / NLP: HuggingFace Transformers

Data Quality: Soda SQL

Testing: Pytest

CI/CD: GitHub Actions

âš™ï¸ Pipeline Workflow
Extract YouTube Data

Fetch all video IDs from MrBeast channel

Pull video metadata

Save raw JSON

Database Setup

PostgreSQL initialized via Docker

Staging and core tables created via hooks and cursors

Transformation Layer

Video duration â†’ short / normal

Title sentiment + sentiment score

Timestamp normalization

Modification Layer

Insert, update, delete logic

Uses row dictionaries returned from transformations

Orchestration

Tasks defined and grouped in Airflow DAGs

Monitored via Airflow UI

Data Quality Checks

Soda scans executed post-load

ğŸ—‚ Airflow DAG Flow
mermaid
Copy code
flowchart TD
    A[Trigger DAG] --> B[Extract YouTube API Data]
    B --> C[Save Raw JSON]
    C --> D[Load to PostgreSQL Staging]
    D --> E[Transform Data]
    E --> F[Video Type Logic]
    E --> G[Sentiment Analysis]
    F --> H[Core Table Insert / Update]
    G --> H
    H --> I[Soda Data Quality Scan]
    I --> J[Unit / Integration / E2E Tests]
    J --> K[DAG Success]
ğŸ“‚ Project Structure
text
Copy code
.
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ youtube_pipeline_dag.py
â”œâ”€â”€ datawarehousing/
â”‚   â”œâ”€â”€ data_utils.py          # DB hooks, connections, AI sentiment
â”‚   â”œâ”€â”€ data_loading.py        # API extraction & raw loading
â”‚   â”œâ”€â”€ transformation.py     # Business + AI transformations
â”‚   â”œâ”€â”€ modification.py       # Insert / update / delete logic
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â”œâ”€â”€ e2e/
â”‚   â””â”€â”€ conftest.py
â”œâ”€â”€ soda/
â”‚   â””â”€â”€ checks.yml
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
ğŸ§ª Data Quality & Testing
ğŸ” Soda Checks
No duplicate records

No missing critical columns

â‰¥ 90% sentiment scores not zero

â‰¥ 90% titles not neutral

ğŸ§ª Testing Levels
Unit Tests: DAG imports, mocks, DB connections

Integration Tests: Real API & PostgreSQL

End-to-End Tests: Full pipeline execution via pytest

ğŸš€ CI/CD Workflow
Implemented using GitHub Actions

Conditional workflows based on file changes:

requirements.txt

DAGs

Soda configs

Docker files

Supports manual workflow dispatch

ğŸ” Environment & Secrets
.env files are not committed

Secrets stored in GitHub Secrets & Variables

Referenced as:

yaml
Copy code
${{ secrets.SECRET_NAME }}
Docker Compose refactored to read from GitHub secrets

âš¡ How to Run
Clone the repository

Run docker-compose up -d

Open Airflow UI at http://localhost:8080

Trigger DAG

Run pytest for validation

âœ… Final Outcome
âœ” End-to-end YouTube ELT pipeline
âœ” AI-enhanced analytics
âœ” Production-grade Airflow orchestration
âœ” Automated data quality & CI/CD
